{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOAhzeNSC9rn5jzysKXhbCS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Unisvet/haf_ai/blob/main/tokenizer_101.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenizer\n",
        "Tokenizers are fundamental components in Natural Language Processing (NLP). They break down text into smaller units called tokens, which can be words, subwords, or characters. This process is crucial for various NLP tasks like text classification, machine translation, and sentiment analysis.\n",
        "\n",
        "Here's how you can get started with tokenizers in Python using the popular `transformers` library:\n",
        "\n",
        "---\n",
        "\n",
        "# 1. Installation\n",
        "\n",
        "First, you need to install the `transformers` library. You can do this using pip:"
      ],
      "metadata": {
        "id": "zSBU2CJSgb0T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46iFyiJuf90-",
        "outputId": "ca7361b0-ed74-4cd7-9905-79bee813bea5",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.31.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Basic Usage\n"
      ],
      "metadata": {
        "id": "WAUIeoNRhD9j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, we load a pre-trained tokenizer for the BERT model. Then, we tokenize a sentence and convert the tokens into numerical IDs, which are essential for feeding into deep learning models.\n",
        "\n",
        "**Steps:**\n",
        "\n",
        "*   We use `AutoTokenizer` to automatically select the appropriate tokenizer based on the model name (`bert-base-uncased`).\n",
        "*   `tokenize()` splits the text into tokens.\n",
        "*   `convert_tokens_to_ids()` maps tokens to their corresponding numerical IDs.\n",
        "\n",
        "Additional Notes\n",
        "\n",
        "The transformers library provides various tokenizers for different models like BERT, GPT-2, RoBERTa, etc.\n",
        "You can explore different pre-trained models and tokenizers on the Hugging Face [link](https://huggingface.co/docs/tokenizers/index).\n",
        "Tokenization is a crucial step in preparing text data for NLP tasks."
      ],
      "metadata": {
        "id": "EtZZ8JYO1jTY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load a pre-trained tokenizer (e.g., BERT)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Tokenize a sentence\n",
        "text = \"This is an example sentence.\"\n",
        "tokens = tokenizer.tokenize(text)\n",
        "\n",
        "# Convert tokens to numerical IDs\n",
        "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "# Print the results\n",
        "print(\"Tokens:\", tokens)\n",
        "print(\"Token IDs:\", token_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_pHFrC6ahIqc",
        "outputId": "f9a63946-41b5-485e-9504-66f8627f96bd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['this', 'is', 'an', 'example', 'sentence', '.']\n",
            "Token IDs: [2023, 2003, 2019, 2742, 6251, 1012]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "# 3. Compare Tokenizers\n",
        "We'll compare the following tokenizers, all from the `transformers` library:\n",
        "\n",
        "* **BERT**: A popular transformer-based model known for its strong performance in various NLP tasks.\n",
        "* **GPT-2**: A powerful language model capable of generating human-like text.\n",
        "* **RoBERTa**: An optimized version of BERT that achieves even better results.\n",
        "* **SentencePiece** (used by XLNet): A subword tokenizer that can handle multiple languages.\n"
      ],
      "metadata": {
        "id": "-231hpOgkVaD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import AutoTokenizer\n",
        "\n",
        "# Define a list of tokenizers to compare\n",
        "tokenizers = [\n",
        "    (\"BERT\", AutoTokenizer.from_pretrained(\"bert-base-uncased\")),\n",
        "    (\"GPT-2\", AutoTokenizer.from_pretrained(\"gpt2\")),\n",
        "    (\"RoBERTa\", AutoTokenizer.from_pretrained(\"roberta-base\")),\n",
        "    (\"SentencePiece\", AutoTokenizer.from_pretrained(\"xlnet-base-cased\")),\n",
        "]\n",
        "\n",
        "# Define a sample text\n",
        "text = \"This is a test sentence for comparing tokenizers.\"\n",
        "\n",
        "# Compare the tokenization results\n",
        "for name, tokenizer in tokenizers:\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    print(f\"{name}: {tokens}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5crU1BbqlxJM",
        "outputId": "a7bae50d-0638-41a6-847e-6f11eb7efc3e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BERT: ['this', 'is', 'a', 'test', 'sentence', 'for', 'comparing', 'token', '##izer', '##s', '.']\n",
            "GPT-2: ['This', 'Ġis', 'Ġa', 'Ġtest', 'Ġsentence', 'Ġfor', 'Ġcomparing', 'Ġtoken', 'izers', '.']\n",
            "RoBERTa: ['This', 'Ġis', 'Ġa', 'Ġtest', 'Ġsentence', 'Ġfor', 'Ġcomparing', 'Ġtoken', 'izers', '.']\n",
            "SentencePiece: ['▁This', '▁is', '▁a', '▁test', '▁sentence', '▁for', '▁comparing', '▁token', 'izer', 's', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code displays the tokens generated by each tokenizer for the given text, allowing you to compare their differences.\n",
        "1. We import AutoTokenizer from transformers for loading tokenizers.\n",
        "2. We create a list of tuples, each containing the name and the loaded tokenizer instance.\n",
        "3. We define a sample text for comparison.\n",
        "4. We iterate through the list of tokenizers, tokenize the sample text using the current tokenizer, print the tokenizer name and the resulting tokens.\n",
        "\n",
        "Additional Notes:\n",
        "\n",
        "You can modify the `text` variable to test with different sentences.\n",
        "Feel free to experiment with other tokenizers available in the `transformers` library. More examples are here [https://huggingface.co/docs/transformers/en/preprocessing](https://huggingface.co/docs/transformers/en/preprocessing)\n",
        "\n",
        "To train your tokenizer look at [https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/tokenizer_training.ipynb](https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/tokenizer_training.ipynb)\n"
      ],
      "metadata": {
        "id": "ELSDHEfVzaid"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PteDTEt-05q_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}