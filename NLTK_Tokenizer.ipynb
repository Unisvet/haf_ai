{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNHygEzG3FIkF5f0n26jARO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Unisvet/haf_ai/blob/main/NLTK_Tokenizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JiiAB0EyLe3r"
      },
      "outputs": [],
      "source": []
    },
    {
      "source": [
        "# 1. Install NLTK (if not already installed)\n",
        "!pip install nltk==3.9.1\n",
        "\n",
        "# 2. Import necessary modules\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import FreqDist\n",
        "\n",
        "# 3. Create a sample text\n",
        "text = \"This is a paragraph about tokenization. Tokenization is a common task in natural language processing. It involves splitting text into individual words or tokens. These tokens are then used for further analysis.\"\n",
        "\n"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wMwrl-o2NLvF",
        "outputId": "c0fa543d-f3dc-42b3-d53d-1985ea599094"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk==3.9.1 in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk==3.9.1) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk==3.9.1) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk==3.9.1) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk==3.9.1) (4.66.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the 'punkt_tab' resource\n",
        "nltk.download('punkt_tab') # Download the necessary data for tokenization.\n",
        "# 4. Tokenize the text\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# 5. Print the tokens\n",
        "print(\"Tokens:\", tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZAH7rEQjNpFI",
        "outputId": "4d8b9b68-4066-4e76-ee57-bde2b23b938c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['This', 'is', 'a', 'paragraph', 'about', 'tokenization', '.', 'Tokenization', 'is', 'a', 'common', 'task', 'in', 'natural', 'language', 'processing', '.', 'It', 'involves', 'splitting', 'text', 'into', 'individual', 'words', 'or', 'tokens', '.', 'These', 'tokens', 'are', 'then', 'used', 'for', 'further', 'analysis', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Count the number of tokens\n",
        "num_tokens = len(tokens)\n",
        "print(\"\\nNumber of tokens:\", num_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lu02brXLN-1u",
        "outputId": "9310602b-5370-4e13-9889-bf4186557596"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of tokens: 36\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#7. Identify the frequency of each token\n",
        "token_freq = FreqDist(tokens)\n",
        "print(\"\\nToken Frequency:\", token_freq.most_common())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pm0y-HlSOEnn",
        "outputId": "a20cca6e-d0b8-4cb2-9419-cd83500af478"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Token Frequency: [('.', 4), ('is', 2), ('a', 2), ('tokens', 2), ('This', 1), ('paragraph', 1), ('about', 1), ('tokenization', 1), ('Tokenization', 1), ('common', 1), ('task', 1), ('in', 1), ('natural', 1), ('language', 1), ('processing', 1), ('It', 1), ('involves', 1), ('splitting', 1), ('text', 1), ('into', 1), ('individual', 1), ('words', 1), ('or', 1), ('These', 1), ('are', 1), ('then', 1), ('used', 1), ('for', 1), ('further', 1), ('analysis', 1)]\n"
          ]
        }
      ]
    }
  ]
}