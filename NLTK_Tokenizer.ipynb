{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOZ9Nb7mz5wyOyw6qm0B29A",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Unisvet/haf_ai/blob/main/NLTK_Tokenizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "FSIIhdYxR9wm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization Example with NLTK\n",
        "\n",
        "This script demonstrates a basic example of tokenization using the Natural Language Toolkit (NLTK) library in Python.\n",
        "\n",
        "**Functionality:**\n",
        "\n",
        "1. **Installation:** Installs the NLTK library using pip if it's not already installed.\n",
        "2. **Import:** Imports necessary modules from NLTK: `word_tokenize` for word tokenization and `FreqDist` for frequency distribution (although not used in this specific example).\n",
        "3. **Sample Text:** Defines a sample text string for tokenization.\n",
        "4. **Data Download:** Downloads the 'punkt_tab' resource using `nltk.download('punkt_tab')`. This resource is essential for sentence tokenization, which is internally used by `word_tokenize`.\n",
        "5. **Tokenization:** Tokenizes the sample text using `word_tokenize` and stores the resulting tokens in a list called `tokens`.\n",
        "6. **Output:** Prints the generated tokens to the console.\n",
        "\n",
        "**Usage:**\n",
        "\n",
        "1. Ensure you have Python and pip installed.\n",
        "2. Run the script. It will install NLTK if necessary, download the required data, and then perform tokenization on the sample text, printing the results.\n",
        "\n",
        "**Dependencies:**\n",
        "\n",
        "- NLTK (version 3.9.1)\n",
        "\n",
        "**Note:**\n",
        "\n",
        "The `punkt_tab` resource download is crucial for this script to function correctly. This resource provides the data required for sentence tokenization, which is a prerequisite for word tokenization using `word_tokenize`."
      ],
      "metadata": {
        "id": "lSw-pkeeRxjI"
      }
    },
    {
      "source": [
        "# 1. Install NLTK (if not already installed)\n",
        "!pip install nltk==3.9.1\n",
        "\n",
        "# 2. Import necessary modules\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import FreqDist\n",
        "\n",
        "# 3. Create a sample text\n",
        "text = \"This is a paragraph about tokenization. Tokenization is a common task in natural language processing. It involves splitting text into individual words or tokens. These tokens are then used for further analysis.\"\n",
        "\n"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wMwrl-o2NLvF",
        "outputId": "c0fa543d-f3dc-42b3-d53d-1985ea599094"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk==3.9.1 in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk==3.9.1) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk==3.9.1) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk==3.9.1) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk==3.9.1) (4.66.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the 'punkt_tab' resource\n",
        "nltk.download('punkt_tab') # Download the necessary data for tokenization.\n",
        "# 4. Tokenize the text\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# 5. Print the tokens\n",
        "print(\"Tokens:\", tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZAH7rEQjNpFI",
        "outputId": "4d8b9b68-4066-4e76-ee57-bde2b23b938c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['This', 'is', 'a', 'paragraph', 'about', 'tokenization', '.', 'Tokenization', 'is', 'a', 'common', 'task', 'in', 'natural', 'language', 'processing', '.', 'It', 'involves', 'splitting', 'text', 'into', 'individual', 'words', 'or', 'tokens', '.', 'These', 'tokens', 'are', 'then', 'used', 'for', 'further', 'analysis', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Count the number of tokens\n",
        "num_tokens = len(tokens)\n",
        "print(\"\\nNumber of tokens:\", num_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lu02brXLN-1u",
        "outputId": "9310602b-5370-4e13-9889-bf4186557596"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of tokens: 36\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#7. Identify the frequency of each token\n",
        "token_freq = FreqDist(tokens)\n",
        "print(\"\\nToken Frequency:\", token_freq.most_common())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pm0y-HlSOEnn",
        "outputId": "a20cca6e-d0b8-4cb2-9419-cd83500af478"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Token Frequency: [('.', 4), ('is', 2), ('a', 2), ('tokens', 2), ('This', 1), ('paragraph', 1), ('about', 1), ('tokenization', 1), ('Tokenization', 1), ('common', 1), ('task', 1), ('in', 1), ('natural', 1), ('language', 1), ('processing', 1), ('It', 1), ('involves', 1), ('splitting', 1), ('text', 1), ('into', 1), ('individual', 1), ('words', 1), ('or', 1), ('These', 1), ('are', 1), ('then', 1), ('used', 1), ('for', 1), ('further', 1), ('analysis', 1)]\n"
          ]
        }
      ]
    }
  ]
}